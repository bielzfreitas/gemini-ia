{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Usar \"colab.new\" no navegador para criar automaticamente um projeto novo.\n",
        "\n",
        "***Atalhos***\n",
        "- CTRL + ENTER (executa uma linha)\n",
        "- SHIFT + ENTER (executa e cria uma nova linha)\n",
        "- CTRL + ESPAÇO (busca a linha de opções que podemos utilizar após o \".\")\n",
        "\n",
        "***Criando um ChatBot usando API Key do IA Studio da Google***\n",
        "- Gerar a key e alterar no campo necessário com a SUA key"
      ],
      "metadata": {
        "id": "OO0wU1UMjQH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Existem versões diferentes do Gemini**\n",
        "\n",
        "Hoje existe o Gemini 1.0 Pro / Gemini 1.5 Pro / Gemini 1.0 Pro Vision\n",
        "\n",
        "`Gemini Pro: fazer solicitações ou enviar prompts somente de texto.`\n",
        "\n",
        "`Gemini Pro Vision: experiencias de multimodalidades, podendo enviar uma imagem, um vídeo e um texto tudo junto.`\n",
        "\n",
        "---\n",
        "\n",
        "`Caso decida optar pelo 1.0 Pro: está apontando para a versão estável dele, ou seja, versão que está mais testada / validada `\n",
        "\n",
        "`Caso decida optar pelo 1.0 Pro 001: criada para ser uma versão experimental, sendo mais usada para solicitações experimentais`\n",
        "\n",
        "`Caso decida optar pelo 1.0 Pro Latest: aponta, com mais facilidade (ao inves de digitar codigo \"001 / 002 / 010\") a versão experimental mais recente, apontando para a 001, pois é a unica disponivel (no momento)`\n",
        "- Importante para validar que os prompts e cenário continue estável e funcionando bem nas novas versões do modelo\n",
        "- garante que os prompts serão executados com precisão\n"
      ],
      "metadata": {
        "id": "IZREQae1rfLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Existem 4 tipos de bloqueio de segurança***\n",
        "- BLOCK_NONE;\n",
        "- BLOCK_FEW;\n",
        "- BLOCK_SOME;\n",
        "- BLOCK_MOST.\n",
        "\n",
        "Todos os bloqueios existentes no gemini: Hate, Harassment, Sexual, Dangerous"
      ],
      "metadata": {
        "id": "VDSOhixgwjBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O \"input\" será usado para entender que está solicitando um prompt de texto para o chatbot responder"
      ],
      "metadata": {
        "id": "hwzCZ--81iOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Organizando o modelo***"
      ],
      "metadata": {
        "id": "7zBC4Lj5xJAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cnP7DwFvg29b"
      },
      "outputs": [],
      "source": [
        "#Instalando o SDK do Google\n",
        "#Gestor de pacotes Python\n",
        "#Usar -q para ter um \"resultado silencioso\" e -U faz update para ultima versão disponivel\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Configurações iniciais\n",
        "#Importando o SDK para podermos usar\n",
        "#\"as genai\" é um apelido para a biblioteca -> intuito de não ficar toda hora chamando o nome completo dela para importar cada uma das ferramentas\n",
        "import google.generativeai as genai\n",
        "\n",
        "#criando um cliente gemini para utilizar\n",
        "# mudar \"INSIRA_SUA_API_KEY\" para a SUA API KEY GERADA NA IA STUDIO!!!!\n",
        "GOOGLE_API_KEY=\"INSIRA_SUA_API_KEY\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "rBFkV0y2jtSZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listando os modelos disponíveis\n",
        "#For -> repetição\n",
        "#m = modelo\n",
        "for m in genai.list_models():\n",
        "  #saber se os metodos de geração suportados pela Google tem o campo \"generateContent\" -> modelos que geram conteudo e imprima o nome do modelo\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "TJ0TV5tTnTQa",
        "outputId": "f5fc2c32-e668-4085-95b6-54900eb3de6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#alterando a temperatuda das respostas do gemini\n",
        "generation_config = {\n",
        "  #dando somente UMA resposta -> mas poderá colocar mais para que o gemini retornar mais de uma resposta\n",
        "  #não existe uma sequencia para criar o codigo\n",
        "  \"candidate_count\": 1,\n",
        "  #coloca a aleatoriedade das palavras (0.5 é ótimo meio termo para as respostas) -> quanto maior, mais criativo o modelo será / quanto menos, mais acertivo (mais fiel ao imput)\n",
        "  \"temperature\": 0.5,\n",
        "}"
      ],
      "metadata": {
        "id": "ePtQS7gQnbS0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#configurações de segurança\n",
        "safety_settings={\n",
        "    'HATE': 'BLOCK_NONE',\n",
        "    'HARASSMENT': 'BLOCK_NONE',\n",
        "    'SEXUAL' : 'BLOCK_NONE',\n",
        "    'DANGEROUS' : 'BLOCK_NONE'\n",
        "    }"
      ],
      "metadata": {
        "id": "8Kl8-vALndHq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Inicializando o modelo***\n"
      ],
      "metadata": {
        "id": "ESLuz6lmynct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saber qual modelo iremos usar\n",
        "#usar a genai para configurar nosso modelo (generativo)\n",
        "#precisamos especificar o nome do modelo que iremos utilizar\n",
        "#usar o mais estável - 1.0 Pro (o 1.5 ainda não possui customização)\n",
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
        "                                  generation_config=generation_config,\n",
        "                                  safety_settings=safety_settings,)"
      ],
      "metadata": {
        "id": "dexy8nrVnd-w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Criando a response - imput (ver se o modelo está gerando conteúdo)***\n"
      ],
      "metadata": {
        "id": "aU-iEBNZ0NV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#aqui iremos usar o modelo -> perguntando ao Gemini e recebendo sua resposta\n",
        "#criando a response - imput (ver se o modelo está gerando conteúdo)\n",
        "response = model.generate_content(\"Vamos aprender coisas sobre IA. Me de sugestões\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "YwXgr-CboFjS",
        "outputId": "13211eba-73dc-4183-d77b-39636e2d5e39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Recursos Online:**\n",
            "\n",
            "* [Coursera](https://www.coursera.org/browse/artificial-intelligence)\n",
            "* [edX](https://www.edx.org/learn/artificial-intelligence)\n",
            "* [Udemy](https://www.udemy.com/topic/artificial-intelligence/)\n",
            "* [MIT OpenCourseWare](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/)\n",
            "* [Google AI Education](https://ai.google/education/)\n",
            "\n",
            "**Livros:**\n",
            "\n",
            "* \"Inteligência Artificial: Uma Abordagem Moderna\" por Stuart Russell e Peter Norvig\n",
            "* \"Aprendizado de Máquina\" por Tom Mitchell\n",
            "* \"Aprendizado Profundo\" por Ian Goodfellow, Yoshua Bengio e Aaron Courville\n",
            "* \"Processamento de Linguagem Natural\" por Christopher Manning, Prabhakar Raghavan e Hinrich Schütze\n",
            "* \"Visão Computacional\" por Richard Szeliski\n",
            "\n",
            "**Cursos de Treinamento:**\n",
            "\n",
            "* [IBM Watson Studio](https://www.ibm.com/cloud/watson-studio/)\n",
            "* [Amazon Web Services (AWS) Machine Learning](https://aws.amazon.com/machine-learning/)\n",
            "* [Microsoft Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/)\n",
            "* [Google Cloud AI Platform](https://cloud.google.com/ai-platform/)\n",
            "* [TensorFlow](https://www.tensorflow.org/)\n",
            "\n",
            "**Eventos e Conferências:**\n",
            "\n",
            "* [Conferência Internacional sobre Inteligência Artificial (IJCAI)](https://ijcai.org/)\n",
            "* [Conferência sobre Sistemas de Processamento de Informação Neural (NeurIPS)](https://neurips.cc/)\n",
            "* [Conferência Internacional sobre Aprendizado de Máquina (ICML)](https://icml.cc/)\n",
            "* [Conferência Internacional sobre Visão Computacional (ICCV)](https://iccv2023.thecvf.com/)\n",
            "* [Conferência Internacional sobre Processamento de Linguagem Natural (ACL)](https://acl2023.org/)\n",
            "\n",
            "**Comunidades e Fóruns:**\n",
            "\n",
            "* [Reddit r/ArtificialIntelligence](https://www.reddit.com/r/artificial/)\n",
            "* [Stack Overflow](https://stackoverflow.com/questions/tagged/artificial-intelligence)\n",
            "* [GitHub](https://github.com/topics/artificial-intelligence)\n",
            "* [Kaggle](https://www.kaggle.com/)\n",
            "* [Meetup](https://www.meetup.com/topics/artificial-intelligence/)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aqui iremos usar o modelo -> perguntando ao Gemini e recebendo sua resposta\n",
        "response = model.generate_content(\"Que empresa criou o modelo de IA Gemini?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cZ3uA9QsnuT7",
        "outputId": "a7890d70-1e41-40d9-d4d7-9454b7e59b7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Criando o Chat para conversar com a Gamini***"
      ],
      "metadata": {
        "id": "5_kIKnkX0yRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ChatBot -> utilizando a model com opção \"start_chat\"\n",
        "#o chat tem um historico, então utilizar o \"history=[]\"\n",
        "# o \"[]\" é uma lista vazia\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "prompt = input('Esperando prompt: ')\n",
        "\n",
        "#ele busca no modelo para pegar uma resposta\n",
        "#gerando uma pergunta, ele vai ter que acessar o contexto aprendido pelo Gemini (o modelo que escolhemos 1.0 Pro) e retornar um texto\n",
        "#Gemini: \"quando eu tiver uma resposta ali no prompt, posso ir buscar o conteúdo\"\n",
        "#criando critério de parada -> só para quando digitar \"fim\"\n",
        "while prompt != \"fim\":\n",
        "  response = chat.send_message(prompt)\n",
        "  print(\"Resposta:\", response.text, '\\n\\n')\n",
        "  prompt = input('Esperando prompt: ')\n",
        "\n",
        "#perguntar algo fora do contexto, a fim de entender o contexto (ele guarda na memoria / historico)\n",
        "# - Qual é a capital do Japão?\n",
        "# - Qual é a comida típica desse país?\n",
        "# - Minha prima nasceu nessa cidade. Qual é a nacionalidade dela?\n",
        "# - Qual é a população dessa cidade?\n",
        "# Terminar - fim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "nRtmB6uG02s0",
        "outputId": "df8db850-bc72-486f-9564-3c5514dda684"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esperando prompt: Qual é a capital do Japão?\n",
            "Resposta: Tóquio \n",
            "\n",
            "\n",
            "Esperando prompt: Qual é a comida típica desse país?\n",
            "Resposta: Sushi \n",
            "\n",
            "\n",
            "Esperando prompt: Minha prima nasceu nessa cidade. Qual é a nacionalidade dela?\n",
            "Resposta: Japonesa \n",
            "\n",
            "\n",
            "Esperando prompt: Qual é a população dessa cidade?\n",
            "Resposta: 13.960.236 (2023) \n",
            "\n",
            "\n",
            "Esperando prompt: fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mostrando nossa variável chat\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ip7Ky6A6pNL",
        "outputId": "efb70408-4937-4fae-96af-24f3b1da1d9b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatSession(\n",
              "    model=genai.GenerativeModel(\n",
              "        model_name='models/gemini-1.0-pro',\n",
              "        generation_config={'candidate_count': 1, 'temperature': 0.5},\n",
              "        safety_settings={<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>},\n",
              "        tools=None,\n",
              "        system_instruction=None,\n",
              "    ),\n",
              "    history=[glm.Content({'parts': [{'text': 'Qual é a capital do Japão?'}], 'role': 'user'}), glm.Content({'parts': [{'text': 'Tóquio'}], 'role': 'model'}), glm.Content({'parts': [{'text': 'Qual é a com...a desse país?'}], 'role': 'user'}), glm.Content({'parts': [{'text': 'Sushi'}], 'role': 'model'}), glm.Content({'parts': [{'text': 'Minha prima ...alidade dela?'}], 'role': 'user'}), glm.Content({'parts': [{'text': 'Japonesa'}], 'role': 'model'}), glm.Content({'parts': [{'text': 'Qual é a pop...dessa cidade?'}], 'role': 'user'}), glm.Content({'parts': [{'text': '13.960.236 (2023)'}], 'role': 'model'})]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mostrando somente o historico usando a variavel chat\n",
        "#é uma lista (array list)\n",
        "chat.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ4lquEn6v2M",
        "outputId": "f5712a3e-875e-4d51-ef42-f120a084acdc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"Qual \\303\\251 a capital do Jap\\303\\243o?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"T\\303\\263quio\"\n",
              " }\n",
              " role: \"model\",\n",
              " parts {\n",
              "   text: \"Qual \\303\\251 a comida t\\303\\255pica desse pa\\303\\255s?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"Sushi\"\n",
              " }\n",
              " role: \"model\",\n",
              " parts {\n",
              "   text: \"Minha prima nasceu nessa cidade. Qual \\303\\251 a nacionalidade dela?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"Japonesa\"\n",
              " }\n",
              " role: \"model\",\n",
              " parts {\n",
              "   text: \"Qual \\303\\251 a popula\\303\\247\\303\\243o dessa cidade?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"13.960.236 (2023)\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Deixando o ChatBot com uma visualização mais bonita***"
      ],
      "metadata": {
        "id": "xW1-QE4g6crJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Melhorando a visualização\n",
        "#Código disponível em https://ai.google.dev/tutorials/python_quickstart#import_packages\n",
        "#Fazendo importações\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "#função \"to_markdown\" -> estrutura para deixar o texto mais bonito (titulos / topicos / negritos)\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "#Imprimindo o histórico\n",
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "  print('-------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "-aeseMWf3ysb",
        "outputId": "0b8af20b-ac67-4795-ea18-989b0c681f34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Qual é a capital do Japão?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: Tóquio"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Qual é a comida típica desse país?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: Sushi"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Minha prima nasceu nessa cidade. Qual é a nacionalidade dela?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: Japonesa"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Qual é a população dessa cidade?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: 13.960.236 (2023)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}